{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68bf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12eacdc2-8a65-40ad-b96f-a98c8c4a6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import simplejson\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76eec8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    file_path = path\n",
    "    df = pd.read_csv(file_path)\n",
    "    #Automatic fixes\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True) #replaces empty strings spacess with NaN\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da6a9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObjectArray = [\n",
    "    {\n",
    "        'path': '/Users/brandon/Desktop/Stroke-synthetic_testing-w-NaN&Text.csv',\n",
    "        'storageId': 'Stroke-synthetic_testing-w-NaN&Text.csv',\n",
    "    },\n",
    "    {\n",
    "        'path': '/Users/brandon/Desktop/Stroke-synthetic_training-w-NaN&Text.csv',\n",
    "        'storageId': 'Stroke-synthetic_training-w-NaN&Text.csv',\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1aebd92c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (546635456.py, line 158)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [43]\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = df.drop(column['name'], axis=1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_encode_nonnumeric(fileObjectArray, target):\n",
    "    df_array = []\n",
    "    for file in fileObjectArray:\n",
    "\n",
    "        df = load_file(file['path'])   \n",
    "        df_array.append(df)\n",
    "\n",
    "    df = pd.concat(df_array)\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "\n",
    "    valid_data_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'bool']\n",
    "\n",
    "    valid = df.select_dtypes(include=valid_data_types)\n",
    "\n",
    "    invalid = df.drop(columns=valid.columns)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for column in invalid.columns:\n",
    "        col = invalid[column]\n",
    "\n",
    "        total_count = col.shape[0]\n",
    "        nan_count = col.isna().sum()\n",
    "\n",
    "        col = col.dropna()\n",
    "        not_nan_count = col.shape[0]\n",
    "\n",
    "        numeric = ~pd.to_numeric(col, errors='coerce').isna()\n",
    "        numeric_count = numeric.sum()\n",
    "\n",
    "        if (numeric_count / total_count) > 0.6:\n",
    "            invalid[column] = pd.to_numeric(col, errors='coerce')\n",
    "            #proposed_transform\n",
    "            t = {\n",
    "                'name': column,\n",
    "                'method': 'mixed_to_numeric',\n",
    "                'values' : {\n",
    "                    'counts': int(numeric_count),\n",
    "                    'percent': float(round(numeric_count / total_count * 100))\n",
    "                },\n",
    "                'items': [\n",
    "                    {'text': 'Convert column to numeric', 'value': 'mixed_to_numeric'},\n",
    "                    {'text': 'Remove column', 'value': 'drop'}\n",
    "                ],\n",
    "                'selection': 'mixed_to_numeric'\n",
    "            }\n",
    "            result.append(t)\n",
    "\n",
    "        else:\n",
    "            list_length = len(list(col.unique()))\n",
    "\n",
    "            print()\n",
    "            if list_length == 1:\n",
    "\n",
    "\n",
    "                t = {\n",
    "                    'name': column,\n",
    "                    'method': 'drop_single',\n",
    "                    'unique_values': list(col.unique()),\n",
    "                    'nan_row_index': list(col[col.isna() == True].index),\n",
    "                    'items': [\n",
    "                        {'text': 'Remove column', 'value': 'drop'},\n",
    "                    ],\n",
    "                    'selection': 'drop'\n",
    "                }       \n",
    "\n",
    "            elif list_length == 2:\n",
    "\n",
    "\n",
    "                #give option to map values\n",
    "                value_map = create_binary_map(list(col.unique()))\n",
    "    \n",
    "                t = {\n",
    "                    'name': column,\n",
    "                    'method': 'one_hot_encode_binary',\n",
    "                    'unique_values': list(col.unique()),\n",
    "                    'nan_row_index': list(col[col.isna() == True].index),\n",
    "                    'items': [\n",
    "                        {'text': 'Convert each unique value to a seperate binary column', 'value': 'one_hot_encode'},\n",
    "                        {'text': 'Encode as a single binary column', 'value': 'binary_encode'},\n",
    "                        {'text': 'Remove column', 'value': 'drop'}\n",
    "                    ],\n",
    "                    'selection': 'one_hot_encode',\n",
    "                    'valueMap': value_map\n",
    "                }     \n",
    "\n",
    "            else:    \n",
    "\n",
    "\n",
    "\n",
    "                t = {\n",
    "                    'name': column,\n",
    "                    'method': 'one_hot_encode',\n",
    "                    'unique_values': list(col.unique()),\n",
    "                    'nan_row_index': list(col[col.isna() == True].index),\n",
    "                    'items': [\n",
    "                        {'text': 'Convert each unique value to a seperate binary column', 'value': 'one_hot_encode'},\n",
    "                        {'text': 'Remove column', 'value': 'drop'},\n",
    "                    ],\n",
    "                    'selection': 'one_hot_encode' if len(list(col.unique())) <=20 else 'drop' #rule to decide if column should be dropped by default\n",
    "                }\n",
    "            #ADD LATER\n",
    "            # if len(t['unique_values']) == 2: #allow for boolean conversion if only two unique values\n",
    "            #     t['items'].insert(1,{'text': 'Convert to boolean', 'value': 'mixed_to_boolean'}) \n",
    "            #     if True in t['unique_values']: #if true is one of the unique values, then use this as default\n",
    "            #         t['selection'] = 'mixed_to_boolean'\n",
    "\n",
    "            result.append(t)\n",
    "    return {'fileAnalysisCombined': result} \n",
    "\n",
    "\n",
    "#EFFECT\n",
    "#none\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55a1fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TRANSFORM\n",
    "def transform_encode_nonnumeric(fileObjectArray, target, transform):\n",
    "    df_array = []\n",
    "    for file in fileObjectArray:\n",
    "\n",
    "        df_sub = load_file(file['path'])\n",
    "        df_sub['storage_id'] = file['storageId']\n",
    "        df_array.append(df_sub)\n",
    "\n",
    "    df = pd.concat(df_array) #combine all files into one dataframe\n",
    "\n",
    "    df.reset_index(inplace=True) #reset index to start at 0\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    for column in transform['data']:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if column['selection'] == 'mixed_to_numeric':\n",
    "            print(column['name'])\n",
    "            df[column['name']] = local_transform_mixed_to_numeric(df[column['name']])\n",
    "\n",
    "        #ADD LATER\n",
    "        # elif t['type'] == 'category_to_binary':\n",
    "        #     df[column] = transform_category_to_binary(df[column], t['map'])\n",
    "        \n",
    "        elif column['selection'] == 'one_hot_encode':\n",
    "\n",
    "            position = df.columns.get_loc(column['name'])     \n",
    "\n",
    "            new_columns = local_transform_one_hot_encode(df[column['name']])\n",
    "\n",
    "            \n",
    "            for new_column in new_columns[0:0]:\n",
    "\n",
    "                df.insert(position, new_column, new_columns[new_column])\n",
    "\n",
    "\n",
    "            df = df.drop(column['name'], axis=1)\n",
    "\n",
    "        elif column['selection'] == 'drop':\n",
    "            df = df.drop(column['name'], axis=1)\n",
    "\n",
    "        elif column['selection'] == 'binary_encode':\n",
    "             #the valueMap property is created on the backend when two unique values exist and maniplated on front end\n",
    "            df[column['name']] = df[column['name']].astype('str').map(column['valueMap']).astype('int')\n",
    "\n",
    "        #ensure target remains at end of file\n",
    "\n",
    "\n",
    "    col_list = list(df.columns)\n",
    "    i = col_list.index(target)\n",
    "    reorder_list = col_list[:i] + col_list[i + 1:] + [target]\n",
    "    df = df[reorder_list]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Split files and save)\n",
    "\n",
    "    df.set_index('index')\n",
    "\n",
    "    print(df.columns.to_list())\n",
    "    result = []\n",
    "\n",
    "    grouped = df.groupby(df.storage_id)\n",
    "    for file in fileObjectArray:\n",
    "        file_index=0\n",
    "        df = grouped.get_group(file['storageId'])\n",
    "        df = df.drop('storage_id', axis=1)\n",
    "\n",
    "        print(file)\n",
    "\n",
    "        file_index += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def local_transform_one_hot_encode(series):\n",
    "    working = pd.Series(series)\n",
    "    working = working.dropna()\n",
    "    index = working.index\n",
    "    \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    vector = working.values.reshape(-1,1)\n",
    "    enc.fit(vector)\n",
    "    trans = enc.transform(vector).toarray()\n",
    "    output = pd.DataFrame(trans, columns=enc.categories_, index=index).add_prefix(series.name + '_').astype('int')\n",
    "    output.columns = output.columns.get_level_values(0) #convert multiindex to single index\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a590b5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4981, 21)\n",
      "\n",
      "['index', 'age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi', 'gender_Female', 'gender_Male', 'ever_married_No', 'ever_married_Yes', 'work_type_Govt_job', 'work_type_Private', 'work_type_Self-employed', 'work_type_children', 'Residence_type_Rural', 'Residence_type_Urban', 'smoking_status_Unknown', 'smoking_status_formerly smoked', 'smoking_status_never smoked', 'Risk_low', 'Risk_int', 'Risk_high', 'smoking_status_smokes', 'storage_id', 'stroke']\n",
      "{'path': '/Users/brandon/Desktop/Stroke-synthetic_testing-w-NaN&Text.csv', 'storageId': 'Stroke-synthetic_testing-w-NaN&Text.csv'}\n",
      "{'path': '/Users/brandon/Desktop/Stroke-synthetic_training-w-NaN&Text.csv', 'storageId': 'Stroke-synthetic_training-w-NaN&Text.csv'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = analyze_encode_nonnumeric(fileObjectArray, 'stroke')\n",
    "\n",
    "transform = analysis['fileAnalysisCombined']\n",
    "\n",
    "transform_encode_nonnumeric(fileObjectArray, 'stroke', {'data': transform})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be39a22-d2b4-4dfc-b7de-9b243d3fd2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b1a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8a32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235c221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb27805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a671993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4614f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f214302c4be4af053e1060f64f7316a672414588e1d594bb2a86016ac778f53c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
